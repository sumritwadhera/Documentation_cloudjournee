{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "\n# Glue Studio Notebook\nYou are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n\n## Available Magics\n|          Magic              |   Type       |                                                                        Description                                                                        |\n|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n| %region                     |  String      |  Specify the AWS region in which to initialize a session                                                                                                  |\n| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X                                                                            |\n| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0)                                |\n| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n| %etl                        |  String      |   Changes the session type to Glue ETL.                                                                                                                   |\n| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n| %stop_session               |              |  Stops the current session.                                                                                                                               |\n| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer                       |",
			"metadata": {
				"editable": false,
				"deletable": false,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": null,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.35 \nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::519852036875:role/service-role/AWSGlueServiceRole-PA-crawler\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: a58bde3d-bab0-404e-b6af-550a3a0a5f9b\nApplying the following default arguments:\n--glue_kernel_version 0.35\n--enable-glue-datacatalog true\nWaiting for session a58bde3d-bab0-404e-b6af-550a3a0a5f9b to get into ready status...\nSession a58bde3d-bab0-404e-b6af-550a3a0a5f9b has been created\n\nUnknownMagic: unknown magic command 'number_of_workers'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "####Creating data souce after crawling tables\nabc_out = glueContext.create_dynamic_frame.from_catalog(\n    database = \"db-combo-practice\",\n    table_name = \"abc_out\"\n)\nref1 = glueContext.create_dynamic_frame.from_catalog(\n    database = \"db-combo-practice\",\n    table_name = \"ref1\"\n)\nref2 = glueContext.create_dynamic_frame.from_catalog(\n    database = \"db-combo-practice\",\n    table_name = \"ref2\"\n)\nref3 = glueContext.create_dynamic_frame.from_catalog(\n    database = \"db-combo-practice\",\n    table_name = \"ref3\"\n)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Dynamic Frame to Spark DataFrame \nsparkDf_abc_out = abc_out.toDF()\nsparkDf_ref1 = ref1.toDF()\nsparkDf_ref2 = ref2.toDF()\nsparkDf_ref3 = ref3.toDF()\n#import concat from functions and lit one,\n#for concate to work,nested bracket structure needed\nfrom  pyspark.sql.functions import concat\nfrom  pyspark.sql.functions import lit\nsparkDf_abc_out = sparkDf_abc_out.withColumn(\"com\",concat(\"subnum\",concat(lit('-'),\"visitid\",concat(lit('-'),\"visitseq\"))))\n#need to drop column name \"statusid_dec\" cause it duplicate in another table and partially filled in abc_out table\nsparkDf_abc_out = sparkDf_abc_out.drop(\"statusid_dec\")\nsparkDf_ref1 = sparkDf_ref1.withColumn(\"com1\",concat(\"subnum\",concat(lit('-'),\"visitid\",concat(lit('-'),\"visitseq\"))))\nsparkDf_ref2 = sparkDf_ref2.withColumn(\"com1\",concat(\"subnum\",concat(lit('-'),\"visitid\",concat(lit('-'),\"visitseq\"))))\nsparkDf_ref3 = sparkDf_ref3.withColumn(\"com1\",concat(\"subnum\",concat(lit('-'),\"visitid\",concat(lit('-'),\"visitseq\"))))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Select columns from spark dataframe\nup_ref1 = sparkDf_ref1.select(\"eamnum\",\"com1\")\nup_ref2 = sparkDf_ref2.select(\"aenum\",\"com1\")\nup_ref3 = sparkDf_ref3.select(\"statusid_dec\",\"com1\")\n#combining abc file and ref1\ncombo = sparkDf_abc_out.join(up_ref1,sparkDf_abc_out.com ==  sparkDf_ref1.com1,\"inner\")\n#combining combo and ref2\ncombo1 = combo.join(up_ref2,combo.com ==  up_ref2.com1,\"inner\")\n#combining combo1 and ref3\ncombo2 = combo1.join(up_ref3,combo1.com ==  up_ref3.com1,\"inner\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_final = combo2.drop(\"com1\")\ndf_final = df_final.withColumn(\"IDENTIFIER\",concat(\"com\",concat(lit('-'),\"aenum\")))\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#now only selected columns are needed\nResult = df_final.select(\"IDENTIFIER\",\"RELDEVA_DEC\",\"RELDEVB_DEC\",\"RELDEVC_DEC\",\"RELPROCA_DEC\",\"RELPROCB_DEC\",\"RELPROCC_DEC\",\"SUBNUM\",\"VISITSEQ\",\"VISITID\",\"AENUM\",\"STATUSID_DEC\",\"EAMNUM\")\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Import Dyanmic DataFrame class\nfrom awsglue.dynamicframe import DynamicFrame\n\n#Convert from Spark Data Frame to Glue Dynamic Frame\nResult_conv = DynamicFrame.fromDF(Result, glueContext, \"convert\")\n#transform for a single file\nretransform = Result_conv.repartition(1)\n\nglueContext.write_dynamic_frame.from_options(\nframe =retransform,\nconnection_type = \"s3\",\nconnection_options = {\"path\":\"s3://aa-bucket-for-custom-jobs/output/\"},\nformat = \"csv\",\nformat_options={\n\"separator\": \",\"\n},\ntransformation_ctx = \"datasink01\"\n)\n#############################\nimport boto3\nclient = boto3.client('s3')\nBUCKET_NAME= \"aa-bucket-for-custom-jobs\"\nPREFIX=\"output/\"\nresponse = client.list_objects(\n    Bucket=BUCKET_NAME,\n    Prefix=PREFIX,\n)\n# print(response)\nname = response[\"Contents\"][0][\"Key\"]\n# print(name)\nclient.copy_object(Bucket=\"aa-bucket-for-custom-jobs\", CopySource=BUCKET_NAME+'/'+name, Key=PREFIX+\"RESULT.csv\")\nclient.delete_object(Bucket=BUCKET_NAME, Key=name)\njob.commit()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "{'ResponseMetadata': {'RequestId': 'RD5A1MDWC3QYZCF5', 'HostId': 'soh8XEeATmrHmrZQk1Tzese3CXceOGkG2WKQ35KStdRqniHFeXRAjUhtZfzgHhXBag7KK6B3wwlh4JNKASJvOQ==', 'HTTPStatusCode': 204, 'HTTPHeaders': {'x-amz-id-2': 'soh8XEeATmrHmrZQk1Tzese3CXceOGkG2WKQ35KStdRqniHFeXRAjUhtZfzgHhXBag7KK6B3wwlh4JNKASJvOQ==', 'x-amz-request-id': 'RD5A1MDWC3QYZCF5', 'date': 'Mon, 31 Oct 2022 07:29:43 GMT', 'server': 'AmazonS3'}, 'RetryAttempts': 0}}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}